{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Building_Your_Neural_Network_Step_By_Step.ipynb","provenance":[],"authorship_tag":"ABX9TyPKu7+KjpdpOkS3TTlnPzE9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UOWPkVY8sYiU"},"source":["# 1. Packages"]},{"cell_type":"code","metadata":{"id":"BGV9FCrDsGKX"},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","from testCases import *\n","from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n","from public_tests import *\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lv0A0v5NsdJo"},"source":["# 2. Initialization"]},{"cell_type":"markdown","metadata":{"id":"WhkXDZWSstv5"},"source":["## 2-1. 2-layer Neural Network"]},{"cell_type":"code","metadata":{"id":"y5T2Z1s2sgj9"},"source":["def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    #(≈ 4 lines of code)\n","    # W1 = ...\n","    # b1 = ...\n","    # W2 = ...\n","    # b2 = ...\n","    # YOUR CODE STARTS HERE\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros((n_y, 1))\n","    # YOUR CODE ENDS HERE\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8hbzCn5smld"},"source":["parameters = initialize_parameters(3,2,1)\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))\n","\n","initialize_parameters_test(initialize_parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PwlCFlbSs6oH"},"source":["## 2-2. L-layer Neural Network"]},{"cell_type":"code","metadata":{"id":"ArJLuf1Hs-Ma"},"source":["def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        #(≈ 2 lines of code)\n","        # parameters['W' + str(l)] = ...\n","        # parameters['b' + str(l)] = ...\n","        # YOUR CODE STARTS HERE\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        # YOUR CODE ENDS HERE\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rle8B7JjtAQe"},"source":["parameters = initialize_parameters_deep([5,4,3])\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))\n","\n","initialize_parameters_deep_test(initialize_parameters_deep)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTi017YEtCku"},"source":["# 3. Forward Propageation Module"]},{"cell_type":"markdown","metadata":{"id":"m8jpkxLhtMBN"},"source":["## 3-1. Linear_forward"]},{"cell_type":"code","metadata":{"id":"xJrlN6bBtGBo"},"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    #(≈ 1 line of code)\n","    # Z = ...\n","    # YOUR CODE STARTS HERE\n","    Z = np.dot(W, A) + b\n","    # YOUR CODE ENDS HERE\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ts6DMYktGGU"},"source":["t_A, t_W, t_b = linear_forward_test_case()\n","t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n","print(\"Z = \" + str(t_Z))\n","\n","linear_forward_test(linear_forward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2XsoSoTtO4K"},"source":["## 3-2. Linear-Activation Forward"]},{"cell_type":"code","metadata":{"id":"p32sYbbvtVnD"},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        #(≈ 2 lines of code)\n","        # Z, linear_cache = ...\n","        # A, activation_cache = ...\n","        # YOUR CODE STARTS HERE\n","        Z, linear_cache = linear_forward(A_prev, W, b)        \n","        A, activation_cache = sigmoid(Z)        \n","        # YOUR CODE ENDS HERE\n","    \n","    elif activation == \"relu\":\n","        #(≈ 2 lines of code)\n","        # Z, linear_cache = ...\n","        # A, activation_cache = ...\n","        # YOUR CODE STARTS HERE\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)        \n","        # YOUR CODE ENDS HERE\n","        \n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMGYmSpAtXUE"},"source":["t_A_prev, t_W, t_b = linear_activation_forward_test_case()\n","\n","t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(t_A))\n","\n","t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(t_A))\n","\n","linear_activation_forward_test(linear_activation_forward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRsHB6YCtZFI"},"source":["## 3-3. L-Layer Model"]},{"cell_type":"code","metadata":{"id":"pOEBviSvtfpC"},"source":["def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- activation value from the output (last) layer\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # number of layers in the neural network\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    # The for loop starts at 1 because layer 0 is the input\n","    for l in range(1, L):\n","        A_prev = A \n","        #(≈ 2 lines of code)\n","        # A, cache = ...\n","        # caches ...\n","        # YOUR CODE STARTS HERE\n","        A, cache = linear_activation_forward(A_prev, \n","                                             parameters['W' + str(l)], \n","                                             parameters['b' + str(l)], \n","                                             activation='relu')        \n","        caches.append(cache)\n","        \n","        # YOUR CODE ENDS HERE\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    #(≈ 2 lines of code)\n","    # AL, cache = ...\n","    # caches ...\n","    # YOUR CODE STARTS HERE\n","    AL, cache = linear_activation_forward(A, \n","                                          parameters['W' + str(L)], \n","                                          parameters['b' + str(L)], \n","                                          activation='sigmoid')\n","    caches.append(cache)    \n","    \n","    # YOUR CODE ENDS HERE\n","          \n","    return AL, caches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCre8gv9tkhF"},"source":["t_X, t_parameters = L_model_forward_test_case_2hidden()\n","t_AL, t_caches = L_model_forward(t_X, t_parameters)\n","\n","print(\"AL = \" + str(t_AL))\n","\n","L_model_forward_test(L_model_forward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NACuIbNutmPO"},"source":["# 4. Cost Function"]},{"cell_type":"code","metadata":{"id":"P1MYBiUXtpSC"},"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    # (≈ 1 lines of code)\n","    # cost = ...\n","    # YOUR CODE STARTS HERE\n","    cost= (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n","    \n","    # YOUR CODE ENDS HERE\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    \n","    assert(cost.shape == ())\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srUkr6mttqoZ"},"source":["t_Y, t_AL = compute_cost_test_case()\n","t_cost = compute_cost(t_AL, t_Y)\n","\n","print(\"Cost: \" + str(t_cost))\n","\n","compute_cost_test(compute_cost)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1YuA_qbStsRJ"},"source":["# 5. Backward Propagation Module"]},{"cell_type":"code","metadata":{"id":"mMdGEvqKtqs7"},"source":["A = np.array([[1, 2], [3, 4]])\n","\n","print('axis=1 and keepdims=True')\n","print(np.sum(A, axis=1, keepdims=True))\n","print('axis=1 and keepdims=False')\n","print(np.sum(A, axis=1, keepdims=False))\n","print('axis=0 and keepdims=True')\n","print(np.sum(A, axis=0, keepdims=True))\n","print('axis=0 and keepdims=False')\n","print(np.sum(A, axis=0, keepdims=False))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VQVXL6mQt0gq"},"source":["## 5-1. Linear Backward"]},{"cell_type":"code","metadata":{"id":"ndkywTjet6ic"},"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    # dW = ...\n","    # db = ... sum by the rows of dZ with keepdims=True\n","    # dA_prev = ...\n","    # YOUR CODE STARTS HERE\n","    dW = np.dot(dZ, cache[0].T) / m\n","    db = np.sum(dZ, axis=1, keepdims=True) / m\n","    dA_prev = np.dot(cache[1].T, dZ)    \n","    \n","    # YOUR CODE ENDS HERE\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wR9czx7bt82M"},"source":["t_dZ, t_linear_cache = linear_backward_test_case()\n","t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n","\n","print(\"dA_prev: \" + str(t_dA_prev))\n","print(\"dW: \" + str(t_dW))\n","print(\"db: \" + str(t_db))\n","\n","linear_backward_test(linear_backward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpJxmY0pt_XB"},"source":["## 5-2. Linear-Activation Backward"]},{"cell_type":"code","metadata":{"id":"0Jz_PN4RuDcH"},"source":["def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        #(≈ 2 lines of code)\n","        # dZ =  ...\n","        # dA_prev, dW, db =  ...\n","        # YOUR CODE STARTS HERE\n","        dZ = relu_backward(dA, activation_cache)        \n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        # YOUR CODE ENDS HERE\n","        \n","    elif activation == \"sigmoid\":\n","        #(≈ 2 lines of code)\n","        # dZ =  ...\n","        # dA_prev, dW, db =  ...\n","        # YOUR CODE STARTS HERE\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        # YOUR CODE ENDS HERE\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arTMt8xzuFDg"},"source":["t_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n","\n","t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n","print(\"With sigmoid: dA_prev = \" + str(t_dA_prev))\n","print(\"With sigmoid: dW = \" + str(t_dW))\n","print(\"With sigmoid: db = \" + str(t_db))\n","\n","t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n","print(\"With relu: dA_prev = \" + str(t_dA_prev))\n","print(\"With relu: dW = \" + str(t_dW))\n","print(\"With relu: db = \" + str(t_db))\n","\n","linear_activation_backward_test(linear_activation_backward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OIzvVR-guHsT"},"source":["## 5-3. L-Model Backward"]},{"cell_type":"code","metadata":{"id":"HVauT8d1uK66"},"source":["def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","    \n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation\n","    #(1 line of code)\n","    # dAL = ...\n","    # YOUR CODE STARTS HERE\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))    \n","    \n","    # YOUR CODE ENDS HERE\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    #(approx. 5 lines)\n","    # current_cache = ...\n","    # dA_prev_temp, dW_temp, db_temp = ...\n","    # grads[\"dA\" + str(L-1)] = ...\n","    # grads[\"dW\" + str(L)] = ...\n","    # grads[\"db\" + str(L)] = ...\n","    # YOUR CODE STARTS HERE\n","    current_cache = caches[-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n","    # YOUR CODE ENDS HERE\n","    \n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        #(approx. 5 lines)\n","        # current_cache = ...\n","        # dA_prev_temp, dW_temp, db_temp = ...\n","        # grads[\"dA\" + str(l)] = ...\n","        # grads[\"dW\" + str(l + 1)] = ...\n","        # grads[\"db\" + str(l + 1)] = ...\n","        # YOUR CODE STARTS HERE\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, 'relu')\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp        \n","        # YOUR CODE ENDS HERE\n","\n","    return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O3cIv4QBuO3o"},"source":["t_AL, t_Y_assess, t_caches = L_model_backward_test_case()\n","grads = L_model_backward(t_AL, t_Y_assess, t_caches)\n","\n","print(\"dA0 = \" + str(grads['dA0']))\n","print(\"dA1 = \" + str(grads['dA1']))\n","print(\"dW1 = \" + str(grads['dW1']))\n","print(\"dW2 = \" + str(grads['dW2']))\n","print(\"db1 = \" + str(grads['db1']))\n","print(\"db2 = \" + str(grads['db2']))\n","\n","L_model_backward_test(L_model_backward)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PkC243nvuRtk"},"source":["# 6. Update Parameters"]},{"cell_type":"code","metadata":{"id":"B7slF8TzuW87"},"source":["def update_parameters(params, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    params -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    parameters = params.copy()\n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    #(≈ 2 lines of code)\n","    for l in range(L):\n","        # parameters[\"W\" + str(l+1)] = ...\n","        # parameters[\"b\" + str(l+1)] = ...\n","        # YOUR CODE STARTS HERE\n","        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]        \n","    \n","        # YOUR CODE ENDS HERE\n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76kfD0HwuZYQ"},"source":["t_parameters, grads = update_parameters_test_case()\n","t_parameters = update_parameters(t_parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(t_parameters[\"W1\"]))\n","print (\"b1 = \"+ str(t_parameters[\"b1\"]))\n","print (\"W2 = \"+ str(t_parameters[\"W2\"]))\n","print (\"b2 = \"+ str(t_parameters[\"b2\"]))\n","\n","update_parameters_test(update_parameters)ㄹ"],"execution_count":null,"outputs":[]}]}