{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trasfer_learning_with_MobileNet_v1.ipynb","provenance":[],"authorship_tag":"ABX9TyOr+XVrp9hzPaeVG1GxTpQd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"A0I5JbsjsO0Z"},"source":["# Transfer Learning with MobileNetV2\n","\n","Welcome to this week's assignment, where you'll be using transfer learning on a pre-trained CNN to build an Alpaca/Not Alpaca classifier!\n","\n","<img src=\"images/alpaca.png\" style=\"width:300px;height:220px;\">\n","\n","A pre-trained model is a network that's already been trained on a large dataset and saved, which allows you to use it to customize your own model cheaply and efficiently. The one you'll be using, MobileNetV2, was designed to provide fast and computationally efficient performance. It's been pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes.\n","\n","By the end of this assignment, you will be able to:\n","\n","- Create a dataset from a directory\n","- Preprocess and augment data using the Sequential API\n","- Adapt a pretrained model to new data and train a classifier using the Functional API and MobileNet\n","- Fine-tune a classifier's final layers to improve accuracy "]},{"cell_type":"markdown","metadata":{"id":"XLb16NhosTeJ"},"source":["# 1 - Packages"]},{"cell_type":"code","metadata":{"id":"n75MgApGsI-r"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import tensorflow.keras.layers as tfl\n","\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r9pCO39HsXm7"},"source":["## 1.1 Create the Dataset and Split it into Training and Validation Sets\n","\n","When training and evaluating deep learning models in Keras, generating a dataset from image files stored on disk is simple and fast. Call `image_data_set_from_directory()` to read from the directory and create both training and validation datasets. \n","\n","If you're specifying a validation split, you'll also need to specify the subset for each portion. Just set the training set to `subset='training'` and the validation set to `subset='validation'`.\n","\n","You'll also set your seeds to match each other, so your training and validation sets don't overlap. :) "]},{"cell_type":"code","metadata":{"id":"xuR_VGI1sW-0"},"source":["BATCH_SIZE = 32\n","IMG_SIZE = (160, 160)\n","directory = \"dataset/\"\n","train_dataset = image_dataset_from_directory(directory,\n","                                             shuffle=True,\n","                                             batch_size=BATCH_SIZE,\n","                                             image_size=IMG_SIZE,\n","                                             validation_split=0.2,\n","                                             subset='training',\n","                                             seed=42)\n","validation_dataset = image_dataset_from_directory(directory,\n","                                             shuffle=True,\n","                                             batch_size=BATCH_SIZE,\n","                                             image_size=IMG_SIZE,\n","                                             validation_split=0.2,\n","                                             subset='validation',\n","                                             seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzqt8Y2Jsksk"},"source":["class_names = train_dataset.class_names\n","\n","plt.figure(figsize=(10, 10))\n","for images, labels in train_dataset.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(images[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[labels[i]])\n","        plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hji1HKNKsnHf"},"source":["# 2 - Preprocess and Augment Training Data\n","\n","You may have encountered `dataset.prefetch` in a previous TensorFlow assignment, as an important extra step in data preprocessing. \n","\n","Using `prefetch()` prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.\n","\n","You can set the number of elements to prefetch manually, or you can use `tf.data.experimental.AUTOTUNE` to choose the parameters automatically. Autotune prompts `tf.data` to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations. \n","\n","To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.  Ahh, so convenient! \n","\n","As always, you're invited to read the official docs, which you can find for data augmentation [here](https://www.tensorflow.org/tutorials/images/data_augmentation)."]},{"cell_type":"code","metadata":{"id":"_XzsYiFTswBS"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKiK2IFTsx0e"},"source":["## Exercise 1 - data_augmenter\n","\n","Implement a function for data augmentation. Use a `Sequential` keras model composed of 2 layers:\n","* `RandomFlip('horizontal')`\n","* `RandomRotation(0.2)`"]},{"cell_type":"code","metadata":{"id":"vIQv31mps3si"},"source":["def data_augmenter():\n","    '''\n","    Create a Sequential model composed of 2 layers\n","    Returns:\n","        tf.keras.Sequential\n","    '''\n","    ### START CODE HERE\n","    data_augmentation = tf.keras.Sequential()\n","    data_augmentation.add(RandomFlip('horizontal'))\n","    data_augmentation.add(RandomRotation(0.2))\n","    ### END CODE HERE\n","    \n","    return data_augmentation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW3oFHDQs8hd"},"source":["augmenter = data_augmenter()\n","\n","assert(augmenter.layers[0].name.startswith('random_flip')), \"First layer must be RandomFlip\"\n","assert augmenter.layers[0].mode == 'horizontal', \"RadomFlip parameter must be horizontal\"\n","assert(augmenter.layers[1].name.startswith('random_rotation')), \"Second layer must be RandomRotation\"\n","assert augmenter.layers[1].factor == 0.2, \"Rotation factor must be 0.2\"\n","print('\\033[92mAll tests passed!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgflmlyotcoD"},"source":["data_augmentation = data_augmenter()\n","\n","for image, _ in train_dataset.take(1):\n","    plt.figure(figsize=(10, 10))\n","    first_image = image[0]\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n","        plt.imshow(augmented_image[0] / 255)\n","        plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJdypecEtezm"},"source":["Next, you'll apply your first tool from the MobileNet application in TensorFlow, to normalize your input. Since you're using a pre-trained model that was trained on the normalization values [-1,1], it's best practice to reuse that standard with tf.keras.applications.mobilenet_v2.preprocess_input.\n","\n","**What you should remember:**\n","\n","* When calling image_data_set_from_directory(), specify the train/val subsets and match the seeds to prevent overlap\n","* Use prefetch() to prevent memory bottlenecks when reading from disk\n","* Give your model more to learn from with simple data augmentations like rotation and flipping.\n","* When using a pretrained model, it's best to reuse the weights it was trained on."]},{"cell_type":"markdown","metadata":{"id":"25WxnpL5txB4"},"source":["# 3 - Using MobileNetV2 for Transfer Learning \n","\n","MobileNetV2 was trained on ImageNet and is optimized to run on mobile and other low-power applications. It's 155 layers deep (just in case you felt the urge to plot the model yourself, prepare for a long journey!) and very efficient for object detection and image segmentation tasks, as well as classification tasks like this one. The architecture has three defining characteristics:\n","\n","*   Depthwise separable convolutions\n","*   Thin input and output bottlenecks between layers\n","*   Shortcut connections between bottleneck layers\n","\n","## 3.1 - Inside a MobileNetV2 Convolutional Building Block\n","\n","MobileNetV2 uses depthwise separable convolutions as efficient building blocks. Traditional convolutions are often very resource-intensive, and  depthwise separable convolutions are able to reduce the number of trainable parameters and operations and also speed up convolutions in two steps: \n","\n","1. The first step calculates an intermediate result by convolving on each of the channels independently. This is the depthwise convolution.\n","\n","2. In the second step, another convolution merges the outputs of the previous step into one. This gets a single result from a single feature at a time, and then is applied to all the filters in the output layer. This is the pointwise convolution, or: **Shape of the depthwise convolution X Number of filters.**\n","\n","This diagram was inspired by the original seen <a href=\"https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html#:~:text=MobileNetV2%20is%20a%20significant%20improvement,object%20detection%20and%20semantic%20segmentation.\">here</a>.</center></caption>\n","\n","Each block consists of an inverted residual structure with a bottleneck at each end. These bottlenecks encode the intermediate inputs and outputs in a low dimensional space, and prevent non-linearities from destroying important information. \n","\n","The shortcut connections, which are similar to the ones in traditional residual networks, serve the same purpose of speeding up training and improving predictions. These connections skip over the intermediate convolutions and connect the bottleneck layers. "]},{"cell_type":"code","metadata":{"id":"6IULKWJUt7Q_"},"source":["IMG_SHAPE = IMG_SIZE + (3,)\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                               include_top=True,\n","                                               weights='imagenet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxE9hPQzZ3g-"},"source":["## 3.2 - Layer Freezing with the Functional API\n","\n","In the next sections, you'll see how you can use a pretrained model to modify the classifier task so that it's able to recognize alpacas. You can achieve this in three steps: \n","\n","1. Delete the top layer (the classification layer)\n","    * Set `include_top` in `base_model` as False\n","2. Add a new classifier layer\n","    * Train only one layer by freezing the rest of the network\n","    * As mentioned before, a single neuron is enough to solve a binary classification problem.\n","3. Freeze the base model and train the newly-created classifier layer\n","    * Set `base model.trainable=False` to avoid changing the weights and train *only* the new layer\n","    * Set training in `base_model` to False to avoid keeping track of statistics in the batch norm layer"]},{"cell_type":"code","metadata":{"id":"w93pII_waZzd"},"source":["def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n","    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n","    Arguments:\n","        image_shape -- Image width and height\n","        data_augmentation -- data augmentation function\n","    Returns:\n","    Returns:\n","        tf.keras.model\n","    '''\n","    \n","    input_shape = image_shape + (3,)\n","    \n","    ### START CODE HERE\n","    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n","                                                   include_top=None, # <== Important!!!!\n","                                                   weights='imagenet') # From imageNet\n","    \n","    # freeze the base model by making it non trainable\n","    base_model.trainable = False\n","\n","    # create the input layer (Same as the imageNetv2 input size)\n","    inputs = tf.keras.Input(shape=input_shape) \n","    \n","    # apply data augmentation to the inputs\n","    x = data_augmentation(inputs)\n","    \n","    # data preprocessing using the same weights the model was trained on\n","    x = preprocess_input(x) \n","    \n","    # set training to False to avoid keeping track of statistics in the batch norm layer\n","    x = base_model(x, training=False) \n","    \n","    # add the new Binary classification layers\n","    # use global avg pooling to summarize the info in each channel\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n","    # include dropout with probability of 0.2 to avoid overfitting\n","    x = tf.keras.layers.Dropout(0.2)(x)\n","        \n","    # use a prediction layer with one neuron (as a binary classifier only needs one)\n","    outputs = tf.keras.layers.Dense(1)(x)\n","    \n","    ### END CODE HERE\n","    \n","    model = tf.keras.Model(inputs, outputs)\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlH1DfSTZ7Q0"},"source":["model2 = alpaca_model(IMG_SIZE, data_augmentation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rqn8kZlwaSMw"},"source":["from test_utils import summary, comparator\n","\n","alpaca_summary = [['InputLayer', [(None, 160, 160, 3)], 0],\n","                    ['Sequential', (None, 160, 160, 3), 0],\n","                    ['TensorFlowOpLayer', [(None, 160, 160, 3)], 0],\n","                    ['TensorFlowOpLayer', [(None, 160, 160, 3)], 0],\n","                    ['Functional', (None, 5, 5, 1280), 2257984],\n","                    ['GlobalAveragePooling2D', (None, 1280), 0],\n","                    ['Dropout', (None, 1280), 0, 0.2],\n","                    ['Dense', (None, 1), 1281, 'linear']] #linear is the default activation\n","\n","comparator(summary(model2), alpaca_summary)\n","\n","for layer in summary(model2):\n","    print(layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XGBr8JjQaT_n"},"source":["base_learning_rate = 0.001\n","model2.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70Uk5lE7nShM"},"source":["## 3.3 - Fine-tuning the Model\n","\n","You could try fine-tuning the model by re-running the optimizer in the last layers to improve accuracy. When you use a smaller learning rate, you take smaller steps to adapt it a little more closely to the new data. In transfer learning, the way you achieve this is by unfreezing the layers at the end of the network, and then re-training your model on the final layers with a very low learning rate. Adapting your learning rate to go over these layers in smaller steps can yield more fine details - and higher accuracy.\n","\n","The intuition for what's happening: when the network is in its earlier stages, it trains on low-level features, like edges. In the later layers, more complex, high-level features like wispy hair or pointy ears begin to emerge. For transfer learning, the low-level features can be kept the same, as they have common features for most images. When you add new data, you generally want the high-level features to adapt to it, which is rather like letting the network learn to detect features more related to your data, such as soft fur or big teeth. \n","\n","To achieve this, just unfreeze the final layers and re-run the optimizer with a smaller learning rate, while keeping all the other layers frozen.\n","\n","Where the final layers actually begin is a bit arbitrary, so feel free to play around with this number a bit. The important takeaway is that the later layers are the part of your network that contain the fine details (pointy ears, hairy tails) that are more specific to your problem.\n","\n","First, unfreeze the base model by setting `base_model.trainable=True`, set a layer to fine-tune from, then re-freeze all the layers before it. Run it again for another few epochs, and see if your accuracy improved!"]},{"cell_type":"code","metadata":{"id":"ZQM1EKs1nd4b"},"source":["base_model = model2.layers[4]\n","base_model.trainable = True\n","# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 120\n","\n","### START CODE HERE\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","    layer.trainable = False\n","    \n","# Define a BinaryCrossentropy loss function. Use from_logits=True\n","loss_function= tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n","optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/10)\n","# Use accuracy as evaluation metric\n","metrics=['accuracy']\n","\n","### END CODE HERE\n","\n","model2.compile(loss=loss_function,\n","              optimizer = optimizer,\n","              metrics=metrics)"],"execution_count":null,"outputs":[]}]}