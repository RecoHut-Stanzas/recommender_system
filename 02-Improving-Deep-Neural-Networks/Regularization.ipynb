{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Regularization.ipynb","provenance":[],"authorship_tag":"ABX9TyPnfDKWXzbkU1i9qypn/MVt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BjlsUL49HBVZ"},"source":["# 1 - Packages "]},{"cell_type":"code","metadata":{"id":"rffULOVlG7He"},"source":["# import packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import sklearn.datasets\n","import scipy.io\n","from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n","from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n","from testCases import *\n","from public_tests import *\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RfybVKsEHFc1"},"source":["# 2 - Problem Statement\n","\n","You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France's goal keeper should kick the ball so that the French team's players can then hit it with their head. \n","\n","<img src=\"images/field_kiank.png\" style=\"width:600px;height:350px;\">\n","\n","<caption><center><font color='purple'><b>Figure 1</b>: Football field. The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head </font></center></caption>\n","\n","\n","They give you the following 2D dataset from France's past 10 games."]},{"cell_type":"markdown","metadata":{"id":"JNJNJCglHQ5v"},"source":["# 3 - Loading the Dataset"]},{"cell_type":"code","metadata":{"id":"e_qfE1TbHFBT"},"source":["train_X, train_Y, test_X, test_Y = load_2D_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKU9YVphHVHT"},"source":["# 4 - Non-Regularized Model"]},{"cell_type":"code","metadata":{"id":"0gpW9EU8HOet"},"source":["def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n","    \"\"\"\n","    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n","    learning_rate -- learning rate of the optimization\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- If True, print the cost every 10000 iterations\n","    lambd -- regularization hyperparameter, scalar\n","    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n","    \n","    Returns:\n","    parameters -- parameters learned by the model. They can then be used to predict.\n","    \"\"\"\n","        \n","    grads = {}\n","    costs = []                            # to keep track of the cost\n","    m = X.shape[1]                        # number of examples\n","    layers_dims = [X.shape[0], 20, 3, 1]\n","    \n","    # Initialize parameters dictionary.\n","    parameters = initialize_parameters(layers_dims)\n","\n","    # Loop (gradient descent)\n","\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n","        if keep_prob == 1:\n","            a3, cache = forward_propagation(X, parameters)\n","        elif keep_prob < 1:\n","            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n","        \n","        # Cost function\n","        if lambd == 0:\n","            cost = compute_cost(a3, Y)\n","        else:\n","            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n","            \n","        # Backward propagation.\n","        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n","                                                # but this assignment will only explore one at a time\n","        if lambd == 0 and keep_prob == 1:\n","            grads = backward_propagation(X, Y, cache)\n","        elif lambd != 0:\n","            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n","        elif keep_prob < 1:\n","            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n","        \n","        # Update parameters.\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        \n","        # Print the loss every 10000 iterations\n","        if print_cost and i % 10000 == 0:\n","            print(\"Cost after iteration {}: {}\".format(i, cost))\n","        if print_cost and i % 1000 == 0:\n","            costs.append(cost)\n","    \n","    # plot the cost\n","    plt.plot(costs)\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (x1,000)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEV85VS2KE_i"},"source":["parameters = model(train_X, train_Y)\n","print (\"On the training set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WukacMmKHk_"},"source":["plt.title(\"Model without regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBf-vjHRKI-L"},"source":["# 5 - L2 Regularization"]},{"cell_type":"code","metadata":{"id":"8Pa0dBLCKUMk"},"source":["# GRADED FUNCTION: compute_cost_with_regularization\n","\n","def compute_cost_with_regularization(A3, Y, parameters, lambd):\n","    \"\"\"\n","    Implement the cost function with L2 regularization. See formula (2) above.\n","    \n","    Arguments:\n","    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    parameters -- python dictionary containing parameters of the model\n","    \n","    Returns:\n","    cost - value of the regularized loss function (formula (2))\n","    \"\"\"\n","    m = Y.shape[1]\n","    W1 = parameters[\"W1\"]\n","    W2 = parameters[\"W2\"]\n","    W3 = parameters[\"W3\"]\n","    \n","    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n","    \n","    #(≈ 1 lines of code)\n","    # L2_regularization_cost = \n","    # YOUR CODE STARTS HERE\n","    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n","      \n","    # YOUR CODE ENDS HERE\n","    \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrL0CAymKWmT"},"source":["A3, t_Y, parameters = compute_cost_with_regularization_test_case()\n","cost = compute_cost_with_regularization(A3, t_Y, parameters, lambd=0.1)\n","print(\"cost = \" + str(cost))\n","\n","compute_cost_with_regularization_test(compute_cost_with_regularization)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVjH3ziYKXhf"},"source":["# GRADED FUNCTION: backward_propagation_with_regularization\n","\n","def backward_propagation_with_regularization(X, Y, cache, lambd):\n","    \"\"\"\n","    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n","    \n","    Arguments:\n","    X -- input dataset, of shape (input size, number of examples)\n","    Y -- \"true\" labels vector, of shape (output size, number of examples)\n","    cache -- cache output from forward_propagation()\n","    lambd -- regularization hyperparameter, scalar\n","    \n","    Returns:\n","    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","    \n","    dZ3 = A3 - Y\n","    #(≈ 1 lines of code)\n","    # dW3 = 1./m * np.dot(dZ3, A2.T) + None\n","    # YOUR CODE STARTS HERE\n","    dW3 = 1. / m * np.dot(dZ3, A2.T) + (lambd * W3) / m\n","    \n","    # YOUR CODE ENDS HERE\n","    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n","    \n","    dA2 = np.dot(W3.T, dZ3)\n","    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","    #(≈ 1 lines of code)\n","    # dW2 = 1./m * np.dot(dZ2, A1.T) + None\n","    # YOUR CODE STARTS HERE\n","    dW2 = 1. / m * np.dot(dZ2, A1.T) + (lambd * W2) / m\n","        \n","    # YOUR CODE ENDS HERE\n","    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","    \n","    dA1 = np.dot(W2.T, dZ2)\n","    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","    #(≈ 1 lines of code)\n","    # dW1 = 1./m * np.dot(dZ1, X.T) + None\n","    # YOUR CODE STARTS HERE\n","    dW1 = 1. / m * np.dot(dZ1, X.T) + (lambd * W1) / m\n","        \n","    # YOUR CODE ENDS HERE\n","    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n","    \n","    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n","                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n","                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","    \n","    return gradients"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9-eI_BBKgcP"},"source":["t_X, t_Y, cache = backward_propagation_with_regularization_test_case()\n","\n","grads = backward_propagation_with_regularization(t_X, t_Y, cache, lambd = 0.7)\n","print (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\n","print (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\n","print (\"dW3 = \\n\"+ str(grads[\"dW3\"]))\n","backward_propagation_with_regularization_test(backward_propagation_with_regularization)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bTRyA4haKjbK"},"source":["parameters = model(train_X, train_Y, lambd = 0.7)\n","print (\"On the train set:\")\n","predictions_train = predict(train_X, train_Y, parameters)\n","print (\"On the test set:\")\n","predictions_test = predict(test_X, test_Y, parameters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-kxK2jQKlDg"},"source":["plt.title(\"Model with L2-regularization\")\n","axes = plt.gca()\n","axes.set_xlim([-0.75,0.40])\n","axes.set_ylim([-0.75,0.65])\n","plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"],"execution_count":null,"outputs":[]}]}