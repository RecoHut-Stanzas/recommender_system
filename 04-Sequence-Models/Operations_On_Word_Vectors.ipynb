{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Operations_On_Word_Vectors.ipynb","provenance":[],"authorship_tag":"ABX9TyPW8tv3E4JmRGRa3CqpT3x5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PiD1mrNMoeYa"},"source":["# Operations on Word Vectors\n","\n","Welcome to your first assignment of Week 2, Course 5 of the Deep Learning Specialization! \n","\n","Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings. In this notebook you'll try your hand at loading, measuring similarity between, and modifying pre-trained embeddings. \n","\n","**After this assignment you'll be able to**:\n","\n","* Explain how word embeddings capture relationships between words\n","* Load pre-trained word vectors\n","* Measure similarity between word vectors using cosine similarity\n","* Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.  \n","\n","At the end of this notebook you'll have a chance to try an optional exercise, where you'll modify word embeddings to reduce their gender bias. Reducing bias is an important consideration in ML, so you're encouraged to take this challenge!  "]},{"cell_type":"markdown","metadata":{"id":"-NKw4oploiNA"},"source":["# 1 - Cosine Similarity\n","\n","To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n","\n","$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n","\n","* $u \\cdot v$ is the dot product (or inner product) of two vectors\n","* $||u||_2$ is the norm (or length) of the vector $u$\n","* $\\theta$ is the angle between $u$ and $v$. \n","* The cosine similarity depends on the angle between $u$ and $v$. \n","    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n","    * If they are dissimilar, the cosine similarity will take a smaller value. \n","\n","### Exercise 1 - cosine_similarity\n","\n","Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n","\n","**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$\n","\n","#### Additional Hints\n","* You may find [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [np.sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html), or [np.sqrt](https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html) useful depending upon the implementation that you choose."]},{"cell_type":"code","metadata":{"id":"79aS7pgEoJ8O"},"source":["def cosine_similarity(u, v):\n","    \"\"\"\n","    Cosine similarity reflects the degree of similarity between u and v\n","        \n","    Arguments:\n","        u -- a word vector of shape (n,)          \n","        v -- a word vector of shape (n,)\n","\n","    Returns:\n","        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n","    \"\"\"\n","    \n","    # Special case. Consider the case u = [0, 0], v=[0, 0]\n","    if np.all(u == v):\n","        return 1\n","    \n","    ### START CODE HERE ###\n","    # Compute the dot product between u and v (≈1 line)\n","    dot = np.dot(u,v)\n","    # Compute the L2 norm of u (≈1 line)\n","    norm_u = np.linalg.norm(u)\n","    \n","    # Compute the L2 norm of v (≈1 line)\n","    norm_v = np.linalg.norm(v)\n","    \n","    # Avoid division by 0\n","    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n","        return 0\n","    \n","    # Compute the cosine similarity defined by formula (1) (≈1 line)\n","    cosine_similarity = dot / (norm_u * norm_v)\n","    ### END CODE HERE ###\n","    \n","    return cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBevzH2cpVi1"},"source":["# 2 - Word Analogy Task\n","\n","* In the word analogy task, complete this sentence:  \n","    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n","\n","* An example is:  \n","    <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n","\n","* You're trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n","    $e_b - e_a \\approx e_d - e_c$\n","* Measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n","\n","<a name='ex-2'></a>\n","### Exercise 2 - complete_analogy\n","\n","Complete the code below to perform word analogies!"]},{"cell_type":"code","metadata":{"id":"CmNqI1vepYVm"},"source":["def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n","    \"\"\"\n","    Performs the word analogy task as explained above: a is to b as c is to ____. \n","    \n","    Arguments:\n","    word_a -- a word, string\n","    word_b -- a word, string\n","    word_c -- a word, string\n","    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n","    \n","    Returns:\n","    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n","    \"\"\"\n","    \n","    # convert words to lowercase\n","    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n","    \n","    ### START CODE HERE ###\n","    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n","    e_a, e_b, e_c = word_to_vec_map.get(word_a), word_to_vec_map.get(word_b), word_to_vec_map.get(word_c)\n","    ### END CODE HERE ###\n","    \n","    words = word_to_vec_map.keys()\n","    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n","    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n","    \n","    # loop over the whole word vector set\n","    for w in words:   \n","        # to avoid best_word being one the input words, skip the input word_c\n","        # skip word_c from query\n","        if w == word_c:\n","            continue\n","        \n","        ### START CODE HERE ###\n","        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n","        cosine_sim = cosine_similarity(np.subtract(e_b,e_a), np.subtract(word_to_vec_map.get(w), e_c))\n","        \n","        # If the cosine_sim is more than the max_cosine_sim seen so far,\n","            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n","        if cosine_sim > max_cosine_sim:\n","            max_cosine_sim = cosine_sim\n","            best_word = w\n","        ### END CODE HERE ###\n","        \n","    return best_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"na74fZKtpzhy"},"source":["def complete_analogy_test(target):\n","    a = [3, 3] # Center at a\n","    a_nw = [2, 4] # North-West oriented vector from a\n","    a_s = [3, 2] # South oriented vector from a\n","    \n","    c = [-2, 1] # Center at c\n","    # Create a controlled word to vec map\n","    word_to_vec_map = {'a': a,\n","                       'synonym_of_a': a,\n","                       'a_nw': a_nw, \n","                       'a_s': a_s, \n","                       'c': c, \n","                       'c_n': [-2, 2], # N\n","                       'c_ne': [-1, 2], # NE\n","                       'c_e': [-1, 1], # E\n","                       'c_se': [-1, 0], # SE\n","                       'c_s': [-2, 0], # S\n","                       'c_sw': [-3, 0], # SW\n","                       'c_w': [-3, 1], # W\n","                       'c_nw': [-3, 2] # NW\n","                      }\n","    \n","    # Convert lists to np.arrays\n","    for key in word_to_vec_map.keys():\n","        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n","            \n","    assert(target('a', 'a_nw', 'c', word_to_vec_map) == 'c_nw')\n","    assert(target('a', 'a_s', 'c', word_to_vec_map) == 'c_s')\n","    assert(target('a', 'synonym_of_a', 'c', word_to_vec_map) != 'c'), \"Best word cannot be input query\"\n","    assert(target('a', 'c', 'a', word_to_vec_map) == 'c')\n","\n","    print(\"\\033[92mAll tests passed\")\n","    \n","complete_analogy_test(complete_analogy)"],"execution_count":null,"outputs":[]}]}