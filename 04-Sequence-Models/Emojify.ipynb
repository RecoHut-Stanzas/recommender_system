{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Emojify.ipynb","provenance":[],"authorship_tag":"ABX9TyMkh8V3J2iMeRsEEN9a7Mse"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"TZXGw2UWrWVK"},"source":["def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n","    \"\"\"\n","    Model to train word vector representations in numpy.\n","    \n","    Arguments:\n","    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n","    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n","    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n","    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n","    num_iterations -- number of iterations\n","    \n","    Returns:\n","    pred -- vector of predictions, numpy-array of shape (m, 1)\n","    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n","    b -- bias of the softmax layer, of shape (n_y,)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","\n","    # Define number of training examples\n","    m = Y.shape[0]                          # number of training examples\n","    n_y = 5                                 # number of classes  \n","    n_h = 50                                # dimensions of the GloVe vectors \n","    \n","    # Initialize parameters using Xavier initialization\n","    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n","    b = np.zeros((n_y,))\n","    \n","    # Convert Y to Y_onehot with n_y classes\n","    Y_oh = convert_to_one_hot(Y, C = n_y) \n","    \n","    # Optimization loop\n","    for t in range(num_iterations):                       # Loop over the number of iterations\n","        for i in range(m):                                # Loop over the training examples\n","            \n","            ### START CODE HERE ### (â‰ˆ 4 lines of code)\n","            # Average the word vectors of the words from the i'th training example\n","            avg = sentence_to_avg(X[i], word_to_vec_map)\n","\n","            # Forward propagate the avg through the softmax layer\n","            z = np.dot(W, avg) + b\n","            a = softmax(z)\n","\n","            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n","            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n","            ### END CODE HERE ###\n","            \n","            # Compute gradients \n","            dz = a - Y_oh[i]\n","            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n","            db = dz\n","\n","            # Update parameters with Stochastic Gradient Descent\n","            W = W - learning_rate * dW\n","            b = b - learning_rate * db\n","        \n","        if t % 100 == 0:\n","            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n","            pred = predict(X, Y, W, b, word_to_vec_map)\n","\n","    return pred, W, b"],"execution_count":null,"outputs":[]}]}