{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dinosaurus_Island_Character_level_language_model.ipynb","provenance":[],"authorship_tag":"ABX9TyNALK/QdrrUkbF0vs0SzJp8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BZmg2Ia2LEHX"},"source":["# 1 - Clip\n","\n","In the exercise below, you will implement a function `clip` that takes in a dictionary of gradients and returns a clipped version of gradients, if needed. \n","\n","* There are different ways to clip gradients.\n","* You will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to fall between some range [-N, N]. \n","* For example, if the N=10\n","    - The range is [-10, 10]\n","    - If any component of the gradient vector is greater than 10, it is set to 10.\n","    - If any component of the gradient vector is less than -10, it is set to -10. \n","    - If any components are between -10 and 10, they keep their original values.\n","\n","## Exercise 1 - clip\n","    \n","Return the clipped gradients of your dictionary `gradients`. \n","    \n","* Your function takes in a maximum threshold and returns the clipped versions of the gradients. \n","* You can check out [numpy.clip](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html) for more info. \n","    - You will need to use the argument \"`out = ...`\".\n","    - Using the \"`out`\" parameter allows you to update a variable \"in-place\".\n","    - If you don't use \"`out`\" argument, the clipped variable is stored in the variable \"gradient\" but does not update the gradient variables `dWax`, `dWaa`, `dWya`, `db`, `dby`."]},{"cell_type":"code","metadata":{"id":"jb9YQXV2K1lw"},"source":["def clip(gradients, maxValue):\n","    '''\n","    Clips the gradients' values between minimum and maximum.\n","    \n","    Arguments:\n","    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n","    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n","    \n","    Returns: \n","    gradients -- a dictionary with the clipped gradients.\n","    '''\n","    gradients = copy.deepcopy(gradients)\n","    \n","    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n","   \n","    ### START CODE HERE ###\n","    # Clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n","    for gradient in [dWax, dWaa, dWya, db, dby]:\n","        np.clip(gradient, -maxValue, maxValue, out=gradient)\n","    ### END CODE HERE ###\n","    \n","    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","    \n","    return gradients"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gApVnFAvMEK9"},"source":["# 2. Sampling\n","\n","Implement the `sample` function below to sample characters. \n","\n","You need to carry out 4 steps:\n","\n","- **Step 1**: Input the \"dummy\" vector of zeros $x^{\\langle 1 \\rangle} = \\vec{0}$. \n","    - This is the default input before you've generated any characters. \n","    You also set $a^{\\langle 0 \\rangle} = \\vec{0}$\n","\n","- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n","\n","*hidden state:*  \n","$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n","\n","*activation:*\n","$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n","\n","*prediction:*\n","$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n","\n","- Details about $\\hat{y}^{\\langle t+1 \\rangle }$:\n","   - Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). \n","   - $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  \n","   - A `softmax()` function is provided for you to use.\n","\n","- **Step 3**: Sampling: \n","    - Now that you have $y^{\\langle t+1 \\rangle}$, you want to select the next letter in the dinosaur name. If you select the most probable, the model will always generate the same result given a starting letter. To make the results more interesting, use `np.random.choice` to select a next letter that is *likely*, but not always the same.\n","    - Pick the next character's **index** according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. \n","    - This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. \n","    - Use [np.random.choice](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html).\n","\n","    Example of how to use `np.random.choice()`:\n","    ```python\n","    np.random.seed(0)\n","    probs = np.array([0.1, 0.0, 0.7, 0.2])\n","    idx = np.random.choice(range(len(probs), p = probs)\n","    ```\n","    \n","    - This means that you will pick the index (`idx`) according to the distribution: \n","\n","    $P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n","\n","    - Note that the value that's set to `p` should be set to a 1D vector.\n","    - Also notice that $\\hat{y}^{\\langle t+1 \\rangle}$, which is `y` in the code, is a 2D array.\n","    - Also notice, while in your implementation, the first argument to `np.random.choice` is just an ordered list [0,1,.., vocab_len-1], it is *not* appropriate to use `char_to_ix.values()`. The *order* of values returned by a Python dictionary `.values()` call will be the same order as they are added to the dictionary. The grader may have a different order when it runs your routine than when you run it in your notebook.\n","\n","- **Step 4**: Update to $x^{\\langle t \\rangle }$ \n","    - The last step to implement in `sample()` is to update the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. \n","    - You will represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character that you have chosen as your prediction. \n","    - You will then forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until you get a `\"\\n\"` character, indicating that you have reached the end of the dinosaur name. "]},{"cell_type":"code","metadata":{"id":"ZyhJbiYdMlM5"},"source":["def sample(parameters, char_to_ix, seed):\n","    \"\"\"\n","    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n","\n","    Arguments:\n","    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n","    char_to_ix -- Python dictionary mapping each character to an index.\n","    seed -- Used for grading purposes. Do not worry about it.\n","\n","    Returns:\n","    indices -- A list of length n containing the indices of the sampled characters.\n","    \"\"\"\n","    \n","    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    vocab_size = by.shape[0]\n","    n_a = Waa.shape[1]\n","    \n","    ### START CODE HERE ###\n","    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n","    # Representing the first character (initializing the sequence generation). (≈1 line)\n","    x = np.zeros((vocab_size, 1))\n","    # Step 1': Initialize a_prev as zeros (≈1 line)\n","    a_prev = np.zeros((n_a, 1))\n","    \n","    # Create an empty list of indices. This is the list which will contain the list of indices of the characters to generate (≈1 line)\n","    indices = []\n","    \n","    # idx is the index of the one-hot vector x that is set to 1\n","    # All other positions in x are zero.\n","    # Initialize idx to -1\n","    idx = -1 \n","    \n","    # Loop over time-steps t. At each time-step:\n","    # Sample a character from a probability distribution \n","    # And append its index (`idx`) to the list \"indices\". \n","    # You'll stop if you reach 50 characters \n","    # (which should be very unlikely with a well-trained model).\n","    # Setting the maximum number of characters helps with debugging and prevents infinite loops. \n","    counter = 0\n","    newline_character = char_to_ix['\\n']\n","    \n","    while (idx != newline_character and counter != 50):\n","        \n","        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n","        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n","        z = np.dot(Wya, a) + by\n","        y = softmax(z)\n","        \n","        # For grading purposes\n","        np.random.seed(counter + seed) \n","        \n","        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n","        # (see additional hints above)\n","        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n","\n","        # Append the index to \"indices\"\n","        indices.append(idx)\n","        \n","        # Step 4: Overwrite the input x with one that corresponds to the sampled index `idx`.\n","        # (see additional hints above)\n","        x = np.zeros((vocab_size, 1))\n","        x[idx] = 1\n","        \n","        # Update \"a_prev\" to be \"a\"\n","        a_prev = a\n","        \n","        # for grading purposes\n","        seed += 1\n","        counter +=1\n","        \n","    ### END CODE HERE ###\n","\n","    if (counter == 50):\n","        indices.append(char_to_ix['\\n'])\n","    \n","    return indices"],"execution_count":null,"outputs":[]}]}