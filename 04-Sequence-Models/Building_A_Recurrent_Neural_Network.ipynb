{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Building_A_Recurrent_Neural_Network.ipynb","provenance":[],"authorship_tag":"ABX9TyPtaBRWARgTxWjWIk2g9Ecf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"TJ6sUfy34khK"},"source":["def rnn_cell_forward(xt, a_prev, parameters):\n","    \"\"\"\n","    Implements a single forward step of the RNN-cell as described in Figure (2)\n","\n","    Arguments:\n","    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n","    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        ba --  Bias, numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","    Returns:\n","    a_next -- next hidden state, of shape (n_a, m)\n","    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n","    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n","    \"\"\"\n","    \n","    # Retrieve parameters from \"parameters\"\n","    Wax = parameters[\"Wax\"]\n","    Waa = parameters[\"Waa\"]\n","    Wya = parameters[\"Wya\"]\n","    ba = parameters[\"ba\"]\n","    by = parameters[\"by\"]\n","    \n","    ### START CODE HERE ### (≈2 lines)\n","    # compute next activation state using the formula given above\n","    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n","    # compute output of the current cell using the formula given above\n","    yt_pred = softmax(np.dot(Wya, a_next) + by)\n","    ### END CODE HERE ###\n","    \n","    # store values you need for backward propagation in cache\n","    cache = (a_next, a_prev, xt, parameters)\n","    \n","    return a_next, yt_pred, cache\n","    \n","\n","def rnn_forward(x, a0, parameters):\n","    \"\"\"\n","    Implement the forward propagation of the recurrent neural network described in Figure (3).\n","\n","    Arguments:\n","    x -- Input data for every time-step, of shape (n_x, m, T_x).\n","    a0 -- Initial hidden state, of shape (n_a, m)\n","    parameters -- python dictionary containing:\n","                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n","                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n","                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n","                        ba --  Bias numpy array of shape (n_a, 1)\n","                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n","\n","    Returns:\n","    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n","    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n","    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n","    \"\"\"\n","    \n","    # Initialize \"caches\" which will contain the list of all caches\n","    caches = []\n","    \n","    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n","    n_x, m, T_x = x.shape\n","    n_y, n_a = parameters[\"Wya\"].shape\n","    \n","    ### START CODE HERE ###\n","    \n","    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n","    a = np.zeros((n_a, m, T_x))\n","    y_pred = np.zeros((n_y, m, T_x))\n","    \n","    # Initialize a_next (≈1 line)\n","    a_next = a0\n","    \n","    # loop over all time-steps\n","    for t in range(T_x):\n","        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n","        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n","        # Save the value of the new \"next\" hidden state in a (≈1 line)\n","        a[:,:,t] = a_next\n","        # Save the value of the prediction in y (≈1 line)\n","        y_pred[:,:,t] = yt_pred\n","        # Append \"cache\" to \"caches\" (≈1 line)\n","        caches.append(cache)\n","    ### END CODE HERE ###\n","    \n","    # store values needed for backward propagation in cache\n","    caches = (caches, x)\n","    \n","    return a, y_pred, caches"],"execution_count":null,"outputs":[]}]}